import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration
import cv2
import mediapipe as mp
import av
import numpy as np
import time
import queue

# ---------------- 1. ê¸°ë³¸ ì„¤ì • ----------------
st.set_page_config(page_title="AI ìë™ ì´¬ì˜ê¸°", layout="centered")

# [ìˆ˜ì •] winsound ì œê±° (ì„œë²„ ì—ëŸ¬ ì›ì¸)
# ì„œë²„ì—ì„œëŠ” st.audioë¡œ ì†Œë¦¬ë¥¼ ì¬ìƒí•´ì•¼ í•©ë‹ˆë‹¤.

# STUN ì„œë²„ (ì™¸ë¶€ ì ‘ì†ìš©)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "snapshot" not in st.session_state:
    st.session_state.snapshot = None
    
# ìš°ì²´í†µ(Queue) ì´ˆê¸°í™”
if "img_queue" not in st.session_state:
    st.session_state.img_queue = queue.Queue()

st.title("ğŸ“¸ AI ìë™ ì´¬ì˜ê¸°")
st.info("ê°ë„ê°€ ë§ìœ¼ë©´ 'CAPTURED' ë©”ì‹œì§€ê°€ ëœ¨ê³  ì‚¬ì§„ì´ ì €ì¥ë©ë‹ˆë‹¤.")

# ---------------- 2. ì‚¬ì´ë“œë°” ì„¤ì • ----------------
st.sidebar.header("âš™ï¸ ì„¤ì •")
min_val = st.sidebar.slider("ìµœì†Œ ê°ë„", 0.0, 0.3, 0.02, 0.01)
max_val = st.sidebar.slider("ìµœëŒ€ ê°ë„", 0.0, 0.3, 0.15, 0.01)

# ---------------- 3. ì˜ìƒ ì²˜ë¦¬ í´ë˜ìŠ¤ ----------------
class FaceMeshProcessor(VideoProcessorBase):
    def __init__(self, img_queue):
        self.img_queue = img_queue
        self.face_mesh = mp.solutions.face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.match_start_time = None
        self.last_capture_time = 0
        self.flash_frame = 0

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1)
        h, w, _ = img.shape
        
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(img_rgb)
        
        status_text = "Adjust Angle"
        color = (0, 0, 255) # ë¹¨ê°•

        # í”Œë˜ì‹œ íš¨ê³¼
        if self.flash_frame > 0:
            self.flash_frame -= 1
            white = np.full((h, w, 3), 255, dtype=np.uint8)
            img = cv2.addWeighted(img, 0.5, white, 0.5, 0)
            status_text = "CAPTURED!"

        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark
            
            # Z-Diff ê³„ì‚°
            chin = landmarks[152].z
            forehead = landmarks[10].z
            current_z = (chin - forehead) * -1 
            
            # ë²”ìœ„ ì²´í¬
            if 0.02 <= current_z <= 0.20:
                color = (0, 255, 0) # ì´ˆë¡
                status_text = "HOLD ON!"
                
                if self.match_start_time is None:
                    self.match_start_time = time.time()
                
                # 1ì´ˆ ìœ ì§€ ì‹œ ì´¬ì˜
                if time.time() - self.match_start_time > 1.0:
                    if time.time() - self.last_capture_time > 3.0:
                        # â˜… ì´¬ì˜ ë° ì „ì†¡ â˜…
                        send_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        self.img_queue.put(send_img)
                        
                        self.last_capture_time = time.time()
                        self.flash_frame = 5
                        # [ìˆ˜ì •] winsound.Beep ì‚­ì œ (ì„œë²„ì—ì„œ ì†Œë¦¬ ëª» ëƒ„)
            else:
                self.match_start_time = None
                
            # ì‹œê°í™”
            cv2.rectangle(img, (0,0), (w,h), color, 15)
            cv2.putText(img, f"Z: {current_z:.4f}", (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)
            cv2.putText(img, status_text, (30, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 3)
            
        return av.VideoFrame.from_ndarray(img, format="bgr24")


# ---------------- 4. ë©”ì¸ ë¡œì§ ----------------

# ì‚¬ì§„ì´ ì°í˜”ìœ¼ë©´ ê²°ê³¼ í™”ë©´ ë³´ì—¬ì£¼ê¸°
if st.session_state.snapshot is not None:
    st.success("ğŸ‰ ì´¬ì˜ ì„±ê³µ!")
    
    # [ì¶”ê°€] ë¸Œë¼ìš°ì €ì—ì„œ ì†Œë¦¬ ì¬ìƒ (ì´ê±´ ì„œë²„ì—ì„œë„ ë¨)
    # ì°°ì¹µ ì†Œë¦¬ íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ í’ì„  íš¨ê³¼ë¡œ ëŒ€ì²´
    st.balloons()
    
    col1, col2 = st.columns(2)
    with col1:
        st.image(st.session_state.snapshot, caption="ì¸ìƒìƒ·", use_container_width=True)
    with col2:
        img_bgr = cv2.cvtColor(st.session_state.snapshot, cv2.COLOR_RGB2BGR)
        ret, buffer = cv2.imencode('.jpg', img_bgr)
        if ret:
            st.download_button(
                label="ğŸ“¥ ê°¤ëŸ¬ë¦¬ì— ì €ì¥",
                data=buffer.tobytes(),
                file_name=f"Selfie_{int(time.time())}.jpg",
                mime="image/jpeg",
                type="primary"
            )
    
    if st.button("ğŸ”„ ë‹¤ì‹œ ì°ê¸°", type="secondary"):
        st.session_state.snapshot = None
        st.rerun()

# ì‚¬ì§„ì´ ì—†ìœ¼ë©´ ì¹´ë©”ë¼ ë³´ì—¬ì£¼ê¸°
else:
    def processor_factory():
        return FaceMeshProcessor(st.session_state.img_queue)

    ctx = webrtc_streamer(
        key="mobile-capture",
        video_processor_factory=processor_factory,
        rtc_configuration=RTC_CONFIGURATION,
        media_stream_constraints={"video": {"facingMode": "user"}, "audio": False},
        async_processing=True
    )

    if ctx.state.playing:
        placeholder = st.empty()
        placeholder.write("ğŸ“¸ ì¹´ë©”ë¼ ì‘ë™ ì¤‘... (ê°ë„ë¥¼ ë§ì¶°ë³´ì„¸ìš”)")
        
        while True:
            if ctx.video_processor:
                try:
                    if not st.session_state.img_queue.empty():
                        result_img = st.session_state.img_queue.get()
                        st.session_state.snapshot = result_img
                        st.rerun()
                except Exception as e:
                    print(e)
            time.sleep(0.1)
