import cv2
import mediapipe as mp
import numpy as np
import av
import math
import time
import queue
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration

# =========================
# 0. ê¸°ë³¸ ì„¸íŒ…
# =========================
st.set_page_config(page_title="AI íƒ€ê²Ÿ êµ¬ë„ ìë™ ì´¬ì˜ê¸°", layout="wide")

RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

mp_face = mp.solutions.face_detection

# ì„¸ì…˜ ê¸°ë³¸ê°’ ì´ˆê¸°í™”
for key, default in [
    ("snapshot", None),   # ì°íŒ ìµœì¢… ì‚¬ì§„
    ("ref_angle", None),  # ê¸°ì¤€ ì‚¬ì§„ì—ì„œ ë‚˜ì˜¨ ê°ë„
    ("angle_tol", 12.0),  # í—ˆìš© ì˜¤ì°¨ (ê¸°ë³¸ 12ë„ ì •ë„ë¡œ ë„ë„í•˜ê²Œ)
]:
    if key not in st.session_state:
        st.session_state[key] = default

if "img_queue" not in st.session_state:
    st.session_state.img_queue = queue.Queue()


# =========================
# 1. ìœ í‹¸ í•¨ìˆ˜
# =========================
def calc_roll_angle_from_detection(detection, width, height):
    """
    Mediapipe FaceDetection ê²°ê³¼ì—ì„œ ì™¼/ì˜¤ë¥¸ìª½ ëˆˆ ìœ„ì¹˜ë¥¼ ì´ìš©í•´
    ì–¼êµ´ roll(ê¸°ìš¸ê¸°) ê°ë„ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜
    """
    keypoints = detection.location_data.relative_keypoints
    left_eye = keypoints[0]
    right_eye = keypoints[1]

    x1, y1 = left_eye.x * width, left_eye.y * height
    x2, y2 = right_eye.x * width, right_eye.y * height

    dx = x2 - x1
    dy = y2 - y1

    angle_rad = math.atan2(dy, dx)
    angle_deg = math.degrees(angle_rad)
    return angle_deg


def analyze_reference_image(uploaded_file):
    """
    ì—…ë¡œë“œëœ ê¸°ì¤€(íƒ€ê²Ÿ) ì‚¬ì§„ì—ì„œ ì–¼êµ´ ê°ë„(roll)ë¥¼ ë¶„ì„í•´ì„œ ê¸°ì¤€ ê°ë„ ë¦¬í„´
    """
    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
    img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
    if img is None:
        return None

    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    h, w, _ = img.shape

    with mp_face.FaceDetection(model_selection=1, min_detection_confidence=0.5) as detector:
        res = detector.process(rgb)
        if res.detections:
            angle = calc_roll_angle_from_detection(res.detections[0], w, h)
            return angle

    return None


# =========================
# 2. WebRTC ì˜ìƒ ì²˜ë¦¬
# =========================
class FaceAngleProcessor(VideoProcessorBase):
    """
    - ê° í”„ë ˆì„ì—ì„œ ì–¼êµ´ ê°ë„ ê³„ì‚°
    - ìµœê·¼ ì—¬ëŸ¬ í”„ë ˆì„ì˜ 'í‰ê·  ê°ë„'ë¥¼ êµ¬í•´ì„œ ì•ˆì •í™”
    - í‰ê·  ê°ë„ê°€ ref_angleê³¼ tolerance ì´ë‚´ë©´ ìë™ ì´¬ì˜
    - ì°íŒ ì‚¬ì§„ì€ img_queueë¡œ ë©”ì¸ ìŠ¤ë ˆë“œì— ì „ë‹¬
    """
    def __init__(self):
        self.ref_angle = None          # ê¸°ì¤€ ê°ë„
        self.tolerance = 12.0          # í—ˆìš© ì˜¤ì°¨
        self.img_queue = None          # ë©”ì¸ ìŠ¤ë ˆë“œë¡œ ë³´ë‚¼ í

        self.detector = mp_face.FaceDetection(
            model_selection=0,
            min_detection_confidence=0.6,
        )
        self.last_capture_time = 0
        self.flash_frame = 0

        # ê°ë„ ì•ˆì •í™”ë¥¼ ìœ„í•œ íˆìŠ¤í† ë¦¬
        self.angle_history = []
        self.max_history = 10  # ìµœê·¼ 10í”„ë ˆì„ê¹Œì§€ë§Œ ì‚¬ìš©

    def _update_angle_history(self, angle):
        self.angle_history.append(angle)
        if len(self.angle_history) > self.max_history:
            self.angle_history.pop(0)

    def _get_smoothed_angle(self):
        """
        ìµœê·¼ angle_historyë¥¼ ì´ìš©í•´ 'í‰ê·  ê°ë„'ë¥¼ ë¦¬í„´
        (ë…¸ì´ì¦ˆ ì¤„ì´ê¸°ìš©)
        """
        if not self.angle_history:
            return None
        return float(sum(self.angle_history) / len(self.angle_history))

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1)  # ê±°ìš¸ ëª¨ë“œ
        h, w, _ = img.shape

        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        res = self.detector.process(img_rgb)

        status_text = "Detecting..."
        color = (0, 0, 255)  # ê¸°ë³¸ ë¹¨ê°•

        # í”Œë˜ì‹œ íš¨ê³¼
        if self.flash_frame > 0:
            self.flash_frame -= 1
            white = np.full((h, w, 3), 255, dtype=np.uint8)
            img = cv2.addWeighted(img, 0.5, white, 0.5, 0)
            status_text = "CAPTURED!"

        if res.detections:
            detection = res.detections[0]
            current_angle = calc_roll_angle_from_detection(detection, w, h)

            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€í•˜ê³ , í‰ê·  ê°ë„ ê³„ì‚°
            self._update_angle_history(current_angle)
            smoothed_angle = self._get_smoothed_angle()

            if smoothed_angle is not None:
                status_text = f"Cur: {current_angle:.1f}Â° / Avg: {smoothed_angle:.1f}Â°"
            else:
                status_text = f"Cur: {current_angle:.1f}Â°"

            # ê¸°ì¤€ ê°ë„ê°€ ìˆì„ ë•Œë§Œ ìë™ ì´¬ì˜ ë¡œì§
            if (self.ref_angle is not None) and (smoothed_angle is not None):
                diff = abs(smoothed_angle - self.ref_angle)
                status_text += f" | Diff: {diff:.1f}Â° (Tol: {self.tolerance:.0f}Â°)"

                # 'í‰ê·  ê°ë„'ê°€ ê¸°ì¤€ ê°ë„ì™€ ì¶©ë¶„íˆ ê°€ê¹Œì›Œì¡Œì„ ë•Œ ì´¬ì˜
                if diff < self.tolerance:
                    color = (0, 255, 0)
                    if time.time() - self.last_capture_time > 3.0:
                        if self.img_queue is not None:
                            save_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                            self.img_queue.put(save_img)
                            self.last_capture_time = time.time()
                            self.flash_frame = 5
                            print("ğŸ“¸ ìë™ ì´¬ì˜ë¨!")

            # ì–¼êµ´ ë°•ìŠ¤ + í…ìŠ¤íŠ¸
            bbox = detection.location_data.relative_bounding_box
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            bw = int(bbox.width * w)
            bh = int(bbox.height * h)
            cv2.rectangle(img, (x, y), (x + bw, y + bh), color, 2)
            cv2.putText(
                img,
                status_text,
                (20, 50),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                color,
                2,
            )

        return av.VideoFrame.from_ndarray(img, format="bgr24")


# =========================
# 3. ë©”ì¸ UI
# =========================
def main():
    st.title("ğŸ“¸ íƒ€ê²Ÿ êµ¬ë„ ë§ì¶”ëŠ” AI ìë™ ì´¬ì˜ê¸°")

    # ì´ë¯¸ í•œ ë²ˆ ì°í˜”ìœ¼ë©´ â†’ ì €ì¥ í™”ë©´
    if st.session_state.get("snapshot") is not None:
        st.success("íƒ€ê²Ÿ êµ¬ë„ì— ë§ê²Œ ì´¬ì˜ ì™„ë£Œ!")

        col1, col2 = st.columns(2)

        with col1:
            st.image(
                st.session_state.snapshot,
                caption="ë°©ê¸ˆ ì°ì€ ì‚¬ì§„",
                use_container_width=True,
            )

        with col2:
            img_bgr = cv2.cvtColor(st.session_state.snapshot, cv2.COLOR_RGB2BGR)
            ret, buffer = cv2.imencode(".jpg", img_bgr)
            if ret:
                st.download_button(
                    label="ğŸ“¥ ì‚¬ì§„ ì €ì¥í•˜ê¸°",
                    data=buffer.tobytes(),
                    file_name=f"Auto_Shot_{int(time.time())}.jpg",
                    mime="image/jpeg",
                    type="primary",
                )

        if st.button("ğŸ”„ ë‹¤ì‹œ ì°ê¸°"):
            st.session_state.snapshot = None
            st.rerun()
        return

    col1, col2 = st.columns([1, 1])

    # -------- ì™¼ìª½: ê¸°ì¤€ ì‚¬ì§„ ì—…ë¡œë“œ --------
    with col1:
        st.subheader("1ï¸âƒ£ íƒ€ê²Ÿ(ê¸°ì¤€) ì‚¬ì§„ ì—…ë¡œë“œ")

        uploaded_file = st.file_uploader(
            "ê¸°ì¤€ ì‚¬ì§„ ì—…ë¡œë“œ (ì–¼êµ´ì´ ì •ë©´/ì¸¡ë©´ì´ë“  í•œ ë²ˆì— ë³´ì´ê²Œ)",
            type=["jpg", "jpeg", "png"],
        )

        if uploaded_file is not None:
            angle = analyze_reference_image(uploaded_file)
            if angle is not None:
                st.session_state.ref_angle = angle
                st.success(f"ê¸°ì¤€ ê°ë„: {angle:.1f}Â° ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.error("ì–¼êµ´ ê°ì§€ ì‹¤íŒ¨. ë‹¤ë¥¸ ì‚¬ì§„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

        ref_angle = st.session_state.get("ref_angle", None)
        if ref_angle is not None:
            st.info(f"í˜„ì¬ ê¸°ì¤€ ê°ë„: {ref_angle:.1f}Â°")
        else:
            st.warning("ê¸°ì¤€ ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ë©´ ê°ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

        # í—ˆìš© ì˜¤ì°¨ ìŠ¬ë¼ì´ë” (ê¸°ë³¸ 12ë„, ë…¸íŠ¸ë¶ì´ë©´ 15~20ë„ê¹Œì§€ë„ ì¶”ì²œ)
        st.session_state.angle_tol = st.slider(
            "í—ˆìš© ê°ë„ ì˜¤ì°¨(ë„)",
            min_value=5.0,
            max_value=25.0,
            value=float(st.session_state.get("angle_tol", 12.0)),
            step=1.0,
            help="ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•´ì•¼ ìë™ ì´¬ì˜í• ì§€ ì •í•˜ëŠ” ê°’ì…ë‹ˆë‹¤.",
        )

    # -------- ì˜¤ë¥¸ìª½: WebRTC ì¹´ë©”ë¼ --------
    with col2:
        st.subheader("2ï¸âƒ£ ì‹¤ì‹œê°„ ì´¬ì˜")

        queue_ref = st.session_state.img_queue

        def processor_factory():
            proc = FaceAngleProcessor()
            proc.ref_angle = st.session_state.get("ref_angle", None)
            proc.tolerance = float(st.session_state.get("angle_tol", 12.0))
            proc.img_queue = queue_ref
            return proc

        ctx = webrtc_streamer(
            key="auto-capture",
            video_processor_factory=processor_factory,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={
                "video": {
                    "width": {"ideal": 640},
                    "height": {"ideal": 480},
                    "facingMode": "user",
                },
                "audio": False,
            },
            async_processing=True,
        )

        # ë””ë²„ê·¸ìš©: ê°•ì œ ìº¡ì³ ë²„íŠ¼ (íŒŒì´í”„ë¼ì¸ í™•ì¸ìš©)
        if ctx.state.playing:
            if st.button("ğŸ’¥ ê°•ì œ ìº¡ì³ (ë””ë²„ê·¸ìš©)"):
                # ê°•ì œë¡œ í•œ í”„ë ˆì„ì„ ìº¡ì³í•˜ëŠ” ê±´ ì–´ë µì§€ë§Œ,
                # ì´ë¯¸ Processorì—ì„œ queueë¡œ ë„£ì–´ì¤€ ê²Œ ìˆìœ¼ë©´ ìš°ì„  ê°€ì ¸ì˜´
                if not st.session_state.img_queue.empty():
                    try:
                        result_img = st.session_state.img_queue.get_nowait()
                        st.session_state.snapshot = result_img
                        st.rerun()
                    except queue.Empty:
                        pass

        # ìë™ ì´¬ì˜ëœ ì‚¬ì§„ ìˆ˜ì‹ 
        if ctx.state.playing:
            if not st.session_state.img_queue.empty():
                try:
                    result_img = st.session_state.img_queue.get_nowait()
                    st.session_state.snapshot = result_img
                    st.rerun()
                except queue.Empty:
                    pass


if __name__ == "__main__":
    main()
