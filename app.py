import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration
import cv2
import mediapipe as mp
import av
import numpy as np
import time
import queue
import math
from PIL import Image

# ---------------- 1. ê¸°ë³¸ ì„¤ì • ----------------
st.set_page_config(page_title="AI ê°ë„ ë”°ë¼ì¡ê¸°", layout="centered")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "snapshot" not in st.session_state:
    st.session_state.snapshot = None
if "target_angle" not in st.session_state:
    st.session_state.target_angle = None
if "target_image" not in st.session_state:
    st.session_state.target_image = None

# Mediapipe ì´ˆê¸°í™” (Face Mesh ì‚¬ìš© - ì •ë°€ë„ ë†’ìŒ)
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# ---------------- 2. í—¬í¼ í•¨ìˆ˜: ê°ë„ ê³„ì‚° ----------------
def calculate_roll_angle(landmarks, img_w, img_h):
    """
    ì™¼ìª½ ëˆˆ(33)ê³¼ ì˜¤ë¥¸ìª½ ëˆˆ(263)ì˜ ì¢Œí‘œë¥¼ ì´ìš©í•´ ì–¼êµ´ì˜ ê¸°ìš¸ê¸°(Roll)ë¥¼ ê³„ì‚°
    """
    left_eye = landmarks[33]
    right_eye = landmarks[263]

    x1, y1 = left_eye.x * img_w, left_eye.y * img_h
    x2, y2 = right_eye.x * img_w, right_eye.y * img_h

    dx = x2 - x1
    dy = y2 - y1

    angle_rad = math.atan2(dy, dx)
    angle_deg = math.degrees(angle_rad)
    
    return angle_deg

# ---------------- 3. ì˜ìƒ ì²˜ë¦¬ í´ë˜ìŠ¤ ----------------
class AngleProcessor(VideoProcessorBase):
    def __init__(self):
        self.result_queue = queue.Queue()
        self.target_angle = None  # ì™¸ë¶€ì—ì„œ ì£¼ì…
        self.frame_count = 0
        self.capture_triggered = False
        self.enter_time = None
        self.flash_frame = 0
        
        # ì„¤ì •ê°’
        self.angle_tolerance = 5.0  # í—ˆìš© ì˜¤ì°¨ (ë„)
        
    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1)  # ê±°ìš¸ ëª¨ë“œ
        h, w, _ = img.shape
        
        # Mediapipe ì²˜ë¦¬
        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb_img)
        
        current_angle = 0.0
        similarity = 0.0
        is_matched = False
        status_msg = "No Face"
        bar_color = (0, 0, 255) # ë¹¨ê°•
        
        # í”Œë˜ì‹œ íš¨ê³¼
        if self.flash_frame > 0:
            self.flash_frame -= 1
            white = np.full((h, w, 3), 255, dtype=np.uint8)
            img = cv2.addWeighted(img, 0.5, white, 0.5, 0)

        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark
            
            # í˜„ì¬ ê°ë„ ê³„ì‚°
            current_angle = calculate_roll_angle(landmarks, w, h)
            
            # íƒ€ê²Ÿ ê°ë„ê°€ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´ ë¹„êµ
            if self.target_angle is not None:
                diff = abs(current_angle - self.target_angle)
                
                # ìœ ì‚¬ë„ ê³„ì‚° (ë‹¨ìˆœí™”: 45ë„ ì°¨ì´ë©´ 0ì , 0ë„ ì°¨ì´ë©´ 100ì )
                max_diff = 45.0
                similarity = max(0, 100 - (diff / max_diff * 100))
                
                status_text = f"Cur: {current_angle:.1f} / Target: {self.target_angle:.1f}"
                cv2.putText(img, status_text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # ë§¤ì¹­ íŒë‹¨
                if diff <= self.angle_tolerance:
                    is_matched = True
                    bar_color = (0, 255, 0) # ì´ˆë¡
                    status_msg = "HOLD!"
                else:
                    status_msg = "Tilt Head"
                    bar_color = (0, 255, 255) if similarity > 70 else (0, 0, 255)

                # ìœ ì‚¬ë„ ê²Œì´ì§€ ë°” ê·¸ë¦¬ê¸°
                bar_x, bar_y, bar_w, bar_h = 20, 80, 200, 20
                cv2.rectangle(img, (bar_x, bar_y), (bar_x + bar_w, bar_y + bar_h), (255, 255, 255), 2)
                fill_w = int(bar_w * (similarity / 100))
                cv2.rectangle(img, (bar_x, bar_y), (bar_x + fill_w, bar_y + bar_h), bar_color, -1)
                cv2.putText(img, f"{int(similarity)}%", (bar_x + bar_w + 10, bar_y + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.6, bar_color, 2)

            # ì´¬ì˜ ì¹´ìš´íŠ¸ë‹¤ìš´ ë¡œì§
            if is_matched:
                if self.enter_time is None:
                    self.enter_time = time.time()
                
                elapsed = time.time() - self.enter_time
                countdown = 1.5 - elapsed
                
                if countdown > 0:
                    cx, cy = w//2, h//2
                    cv2.putText(img, f"{countdown:.1f}", (cx-50, cy+20), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 255), 4)
                else:
                    # ì´¬ì˜!
                    if not self.capture_triggered:
                        save_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # RGBë¡œ ì €ì¥
                        self.result_queue.put(save_img)
                        self.capture_triggered = True
                        self.flash_frame = 5
            else:
                self.enter_time = None
                self.capture_triggered = False

        return av.VideoFrame.from_ndarray(img, format="bgr24")


# ---------------- 4. UI êµ¬ì„± ----------------
st.title("ğŸ“¸ AI ê°ë„ ë”°ë¼ì¡ê¸°")
st.markdown("ë”°ë¼í•˜ê³  ì‹¶ì€ **'ê¸°ì¤€ ì‚¬ì§„'**ì„ ì˜¬ë¦¬ë©´, ê°™ì€ ê°ë„ê°€ ë˜ì—ˆì„ ë•Œ ìë™ìœ¼ë¡œ ì°ì–´ì¤ë‹ˆë‹¤!")

# 4-1. ê¸°ì¤€ ì‚¬ì§„ ì—…ë¡œë“œ ì„¹ì…˜ (ì´¬ì˜ ì „)
if st.session_state.snapshot is None:
    with st.expander("1. ê¸°ì¤€ ì‚¬ì§„ ì—…ë¡œë“œ (Click to Open)", expanded=(st.session_state.target_angle is None)):
        uploaded_file = st.file_uploader("ë”°ë¼í•  ì‚¬ì§„ì„ ì˜¬ë ¤ì£¼ì„¸ìš” (ì •ë©´/ê¸°ìš¸ì¸ ì–¼êµ´)", type=["jpg", "png", "jpeg"])
        
        if uploaded_file is not None:
            # íŒŒì¼ ì½ê¸° ë° ë¶„ì„
            image = Image.open(uploaded_file)
            img_array = np.array(image)
            
            # Mediapipe ë¶„ì„ì„ ìœ„í•´ RGB ë³€í™˜
            if img_array.shape[2] == 4: # PNG alpha channel ì²˜ë¦¬
                img_array = cv2.cvtColor(img_array, cv2.COLOR_RGBA2RGB)
            elif len(img_array.shape) == 2: # Grayscale ì²˜ë¦¬
                img_array = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)
                
            results = face_mesh.process(img_array)
            
            if results.multi_face_landmarks:
                h, w, _ = img_array.shape
                landmarks = results.multi_face_landmarks[0].landmark
                angle = calculate_roll_angle(landmarks, w, h)
                
                st.session_state.target_angle = angle
                st.session_state.target_image = image
                st.success(f"âœ… ê¸°ì¤€ ì‚¬ì§„ ë¶„ì„ ì™„ë£Œ! ëª©í‘œ ê°ë„: {angle:.1f}ë„")
                st.image(image, caption=f"ê¸°ì¤€ ì‚¬ì§„ (ê°ë„: {angle:.1f})", width=200)
            else:
                st.error("ì‚¬ì§„ì—ì„œ ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì‚¬ì§„ì„ ì‹œë„í•´ì£¼ì„¸ìš”.")

# 4-2. ê²°ê³¼ í™”ë©´ (ì´¬ì˜ í›„)
if st.session_state.snapshot is not None:
    st.markdown("---")
    st.success("ğŸ“¸ ì°°ì¹µ! ì´¬ì˜ì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤.")
    
    col1, col2 = st.columns(2)
    with col1:
        st.image(st.session_state.target_image, caption="ê¸°ì¤€ ì‚¬ì§„", use_container_width=True)
    with col2:
        st.image(st.session_state.snapshot, caption="ë‚´ ì‚¬ì§„", use_container_width=True)
        
    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    img_bgr = cv2.cvtColor(st.session_state.snapshot, cv2.COLOR_RGB2BGR)
    is_success, buffer = cv2.imencode(".jpg", img_bgr)
    
    if is_success:
        st.download_button(
            label="ğŸ“¥ ê²°ê³¼ ì‚¬ì§„ ì €ì¥í•˜ê¸°",
            data=buffer.tobytes(),
            file_name=f"AI_Shot_{int(time.time())}.jpg",
            mime="image/jpeg",
            type="primary",
            use_container_width=True
        )
    
    # [ìˆ˜ì •ë¨] ë‹¤ì‹œ ì°ê¸° ë²„íŠ¼ ì‚­ì œ í›„ ì•ˆë‚´ ë¬¸êµ¬ ì¶”ê°€
    st.warning("ğŸ”„ ë‹¤ì‹œ ì´¬ì˜í•˜ì‹œë ¤ë©´ ì›¹í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨ í•´ì£¼ì„¸ìš”.")

# 4-3. ì´¬ì˜ í™”ë©´ (WebRTC)
elif st.session_state.target_angle is not None:
    st.markdown("---")
    st.header("2. ì¹´ë©”ë¼ë¥¼ ë³´ê³  ê°ë„ë¥¼ ë§ì¶°ë³´ì„¸ìš”!")
    
    ctx = webrtc_streamer(
        key="angle-shooter",
        video_processor_factory=AngleProcessor,
        rtc_configuration=RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}),
        media_stream_constraints={"video": {"facingMode": "user"}, "audio": False},
    )
    
    # ì‹¤ì‹œê°„ìœ¼ë¡œ íƒ€ê²Ÿ ê°ë„ ì •ë³´ë¥¼ í”„ë¡œì„¸ì„œì— ì „ë‹¬
    if ctx.video_processor:
        ctx.video_processor.target_angle = st.session_state.target_angle
        
    # ê²°ê³¼ ìˆ˜ì‹  ëŒ€ê¸°
    if ctx.state.playing:
        while True:
            if ctx.video_processor:
                try:
                    result_img = ctx.video_processor.result_queue.get(timeout=0.1)
                    if result_img is not None:
                        st.session_state.snapshot = result_img
                        st.rerun()
                except queue.Empty:
                    pass
            time.sleep(0.1)

else:
    st.info("ğŸ‘† ë¨¼ì € ìœ„ì—ì„œ 'ê¸°ì¤€ ì‚¬ì§„'ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
