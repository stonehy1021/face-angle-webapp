import cv2
import mediapipe as mp
import numpy as np
import av
import math
import time
import queue
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration

# ========= 1. ê¸°ë³¸ ì„¤ì • =========
st.set_page_config(page_title="AI ìë™ ì´¬ì˜ê¸°", layout="wide")

# STUN ì„œë²„ (ì™¸ë¶€ ì ‘ì† í•„ìˆ˜ ì„¤ì •)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì°ì€ ì‚¬ì§„ ì €ì¥ìš©)
if "snapshot" not in st.session_state:
    st.session_state.snapshot = None

# ìš°ì²´í†µ(Queue) ì´ˆê¸°í™” (ì˜ìƒ ì²˜ë¦¬ê¸° -> ë©”ì¸ í™”ë©´ ë°ì´í„° ì „ì†¡ìš©)
if "img_queue" not in st.session_state:
    st.session_state.img_queue = queue.Queue()

# Mediapipe ì„¤ì •
mp_face = mp.solutions.face_detection

# ========= 2. ìœ í‹¸ í•¨ìˆ˜ (ê°ë„ ê³„ì‚°) =========
def calc_roll_angle_from_detection(detection, width, height):
    """ëˆˆ ìœ„ì¹˜ë¡œ Roll ê°ë„ ê³„ì‚° (ì£¼ì‹  ì½”ë“œ ë¡œì§ ìœ ì§€)"""
    keypoints = detection.location_data.relative_keypoints
    left_eye = keypoints[0]
    right_eye = keypoints[1]

    x1, y1 = left_eye.x * width, left_eye.y * height
    x2, y2 = right_eye.x * width, right_eye.y * height

    dx = x2 - x1
    dy = y2 - y1

    angle_rad = math.atan2(dy, dx)
    angle_deg = math.degrees(angle_rad)
    return angle_deg

def analyze_reference_image(uploaded_file):
    """ê¸°ì¤€ ì‚¬ì§„ ë¶„ì„"""
    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
    img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
    if img is None: return None
    
    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    h, w, _ = img.shape
    
    with mp_face.FaceDetection(model_selection=1, min_detection_confidence=0.5) as detector:
        res = detector.process(rgb)
        if res.detections:
            return calc_roll_angle_from_detection(res.detections[0], w, h)
    return None

# ========= 3. ì˜ìƒ ì²˜ë¦¬ í´ë˜ìŠ¤ (í•µì‹¬) =========
class FaceAngleProcessor(VideoProcessorBase):
    def __init__(self):
        self.ref_angle = None  # ê¸°ì¤€ ê°ë„ (ì™¸ë¶€ì—ì„œ ì£¼ì…)
        self.img_queue = None  # ë°ì´í„° ì „ì†¡ í†µë¡œ
        self.detector = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.6)
        
        # ìë™ ì´¬ì˜ ë³€ìˆ˜
        self.last_capture_time = 0
        self.flash_frame = 0

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1) # ê±°ìš¸ ëª¨ë“œ
        h, w, _ = img.shape
        
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        res = self.detector.process(img_rgb)
        
        current_angle = 0.0
        status_text = "Detecting..."
        color = (0, 0, 255) # ë¹¨ê°•

        # í”Œë˜ì‹œ íš¨ê³¼ (ì´¬ì˜ ì§í›„)
        if self.flash_frame > 0:
            self.flash_frame -= 1
            white = np.full((h, w, 3), 255, dtype=np.uint8)
            img = cv2.addWeighted(img, 0.5, white, 0.5, 0)
            status_text = "CAPTURED!"

        if res.detections:
            detection = res.detections[0]
            current_angle = calc_roll_angle_from_detection(detection, w, h)
            
            status_text = f"Cur: {current_angle:.1f}"
            
            # ê¸°ì¤€ ê°ë„ê°€ ìˆìœ¼ë©´ ë¹„êµ ì‹œì‘
            if self.ref_angle is not None:
                diff = abs(current_angle - self.ref_angle)
                status_text += f" | Diff: {diff:.1f}"
                
                # â˜… ì˜¤ì°¨ 5ë„ ì´ë‚´ë©´ ì´¬ì˜ â˜…
                if diff < 5.0:
                    color = (0, 255, 0) # ì´ˆë¡
                    
                    # 3ì´ˆ ì¿¨íƒ€ì„ ì²´í¬
                    if time.time() - self.last_capture_time > 3.0:
                        # ì‚¬ì§„ ì°ì–´ì„œ íì— ë„£ê¸°
                        if self.img_queue is not None:
                            # OpenCV(BGR) -> RGB ë³€í™˜í•´ì„œ ì „ì†¡
                            save_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                            self.img_queue.put(save_img)
                            
                            self.last_capture_time = time.time()
                            self.flash_frame = 5
                            print("ğŸ“¸ ìë™ ì´¬ì˜ë¨!")
            
            # ê·¸ë¦¬ê¸°
            bbox = detection.location_data.relative_bounding_box
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            bw = int(bbox.width * w)
            bh = int(bbox.height * h)
            cv2.rectangle(img, (x, y), (x+bw, y+bh), color, 2)
            cv2.putText(img, status_text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

        return av.VideoFrame.from_ndarray(img, format="bgr24")

# ========= 4. ë©”ì¸ UI =========
def main():
    st.title("ğŸ“¸ AI ìë™ ì´¬ì˜ê¸° (WebRTC)")
    
    col1, col2 = st.columns([1, 1])

    # [ì™¼ìª½] ê¸°ì¤€ ì‚¬ì§„ ì„¤ì •
    with col1:
        st.subheader("1ï¸âƒ£ ê¸°ì¤€ ì‚¬ì§„")
        uploaded_file = st.file_uploader("ê¸°ì¤€ ì‚¬ì§„ ì—…ë¡œë“œ", type=['jpg', 'png'])
        ref_angle_val = None
        
        if uploaded_file:
            angle = analyze_reference_image(uploaded_file)
            if angle is not None:
                ref_angle_val = angle
                st.success(f"ê¸°ì¤€ ê°ë„: {angle:.1f}Â°")
            else:
                st.error("ì–¼êµ´ ê°ì§€ ì‹¤íŒ¨")

    # [ì˜¤ë¥¸ìª½] ì¹´ë©”ë¼ ì‹¤í–‰
    with col2:
        st.subheader("2ï¸âƒ£ ì‹¤ì‹œê°„ ì´¬ì˜")
        
        # Processor Factory ìƒì„± (í ì£¼ì…)
        def processor_factory():
            proc = FaceAngleProcessor()
            proc.ref_angle = ref_angle_val  # ê¸°ì¤€ ê°ë„ ì „ë‹¬
            proc.img_queue = st.session_state.img_queue # ìš°ì²´í†µ ì „ë‹¬
            return proc

        ctx = webrtc_streamer(
            key="auto-capture",
            video_processor_factory=processor_factory,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={"video": {"facingMode": "user"}, "audio": False},
            async_processing=True
        )

        # â˜… í•µì‹¬: ì‹¤ì‹œê°„ìœ¼ë¡œ ìš°ì²´í†µ í™•ì¸í•˜ê¸° â˜…
        if ctx.state.playing:
            if not st.session_state.img_queue.empty():
                try:
                    # ì‚¬ì§„ êº¼ë‚´ê¸°
                    result_img = st.session_state.img_queue.get_nowait()
                    st.session_state.snapshot = result_img
                    st.rerun() # í™”ë©´ ìƒˆë¡œê³ ì¹¨
                except queue.Empty:
                    pass

    # [í•˜ë‹¨] ê²°ê³¼ë¬¼ & ë‹¤ìš´ë¡œë“œ
    st.markdown("---")
    if st.session_state.snapshot is not None:
        st.success("ğŸ‰ ì´¬ì˜ ì„±ê³µ!")
        st.image(st.session_state.snapshot, caption="ë°©ê¸ˆ ì°ì€ ì‚¬ì§„", width=400)
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        img_bgr = cv2.cvtColor(st.session_state.snapshot, cv2.COLOR_RGB2BGR)
        ret, buffer = cv2.imencode('.jpg', img_bgr)
        if ret:
            st.download_button(
                label="ğŸ“¥ ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ",
                data=buffer.tobytes(),
                file_name=f"Auto_Shot_{int(time.time())}.jpg",
                mime="image/jpeg",
                type="primary"
            )
            
        if st.button("ğŸ”„ ë‹¤ì‹œ ì°ê¸°"):
            st.session_state.snapshot = None
            st.rerun()

if __name__ == "__main__":
    main()
