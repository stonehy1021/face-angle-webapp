import cv2
import mediapipe as mp
import numpy as np
import av
import math
import time
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration

# ========= ê¸°ë³¸ ì„¤ì • =========
st.set_page_config(page_title="ì–¼êµ´ ê°ë„ ë¶„ì„", layout="wide")

# STUN ì„œë²„ ì„¤ì • (ë°°í¬ ì‹œ í•„ìˆ˜)
# êµ¬ê¸€ì˜ ë¬´ë£Œ STUN ì„œë²„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì™¸ë¶€ ì ‘ì† í—ˆìš©
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# Mediapipe ì´ˆê¸°í™”
mp_face = mp.solutions.face_detection

# ========= ìœ í‹¸ í•¨ìˆ˜ =========
def calc_roll_angle(detection, width, height):
    """ì–¼êµ´ì˜ ê¸°ìš¸ê¸°(Roll) ê³„ì‚°"""
    kp = detection.location_data.relative_keypoints
    left_eye = kp[0]  # ì™¼ìª½ ëˆˆ
    right_eye = kp[1] # ì˜¤ë¥¸ìª½ ëˆˆ

    x1, y1 = left_eye.x * width, left_eye.y * height
    x2, y2 = right_eye.x * width, right_eye.y * height

    # ê°ë„ ê³„ì‚° (dy, dx)
    angle = math.degrees(math.atan2(y2 - y1, x2 - x1))
    return angle

# ========= ê¸°ì¤€ ì‚¬ì§„ ë¶„ì„ í•¨ìˆ˜ =========
def analyze_static_image(uploaded_file):
    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
    img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
    
    if img is None:
        return None, None

    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    h, w, _ = img.shape

    with mp_face.FaceDetection(model_selection=1, min_detection_confidence=0.5) as detector:
        res = detector.process(img_rgb)
        
        if res.detections:
            angle = calc_roll_angle(res.detections[0], w, h)
            return angle, img
    
    return None, img

# ========= ì˜ìƒ ì²˜ë¦¬ í´ë˜ìŠ¤ (WebRTC í•µì‹¬) =========
class FaceAngleProcessor(VideoProcessorBase):
    def __init__(self):
        self.ref_angle = None # ê¸°ì¤€ ê°ë„
        self.face_detector = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.5)

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        # 1. ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        img = frame.to_ndarray(format="bgr24")
        
        # 2. ì¢Œìš° ë°˜ì „ (ê±°ìš¸ ëª¨ë“œ)
        img = cv2.flip(img, 1)
        h, w, _ = img.shape
        
        # 3. Mediapipe ë¶„ì„ì„ ìœ„í•´ RGB ë³€í™˜
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.face_detector.process(img_rgb)
        
        current_angle = 0.0
        diff = 0.0
        status_text = "No Face"
        color = (0, 0, 255) # ë¹¨ê°•

        if results.detections:
            detection = results.detections[0]
            current_angle = calc_roll_angle(detection, w, h)
            
            status_text = f"Angle: {current_angle:.1f}"
            color = (255, 0, 0) # íŒŒë‘

            # ê¸°ì¤€ ê°ë„ê°€ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´ ì°¨ì´ ê³„ì‚°
            if self.ref_angle is not None:
                diff = abs(current_angle - self.ref_angle)
                status_text += f" | Diff: {diff:.1f}"
                
                # ì°¨ì´ê°€ 5ë„ ì´ë‚´ë©´ ì´ˆë¡ìƒ‰
                if diff < 5.0:
                    color = (0, 255, 0)
                    status_text += " (MATCH!)"

            # ì‹œê°í™” (ë°•ìŠ¤ ë° í…ìŠ¤íŠ¸)
            bbox = detection.location_data.relative_bounding_box
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            bw = int(bbox.width * w)
            bh = int(bbox.height * h)
            
            cv2.rectangle(img, (x, y), (x+bw, y+bh), color, 2)
            cv2.putText(img, status_text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
            
        else:
            cv2.putText(img, "Face Not Found", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

        # 4. ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ì†¡ì¶œ
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# ========= ë©”ì¸ UI =========
def main():
    st.title("ğŸ“¸ AI ì–¼êµ´ ê°ë„ ë¶„ì„ê¸°")
    st.info("ì™¼ìª½ì—ì„œ ê¸°ì¤€ ì‚¬ì§„ì„ ì˜¬ë¦¬ê³ , ì•„ë˜ì—ì„œ ì¹´ë©”ë¼ë¥¼ ì¼œì„¸ìš”.")

    col1, col2 = st.columns([1, 2])

    # [ì™¼ìª½] ê¸°ì¤€ ì‚¬ì§„ ì—…ë¡œë“œ
    with col1:
        st.subheader("1. ê¸°ì¤€ ì‚¬ì§„")
        uploaded_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=['jpg', 'png', 'jpeg'])
        
        ref_angle_val = None
        
        if uploaded_file:
            angle, processed_img = analyze_static_image(uploaded_file)
            if angle is not None:
                ref_angle_val = angle
                st.success(f"ê¸°ì¤€ ê°ë„: {angle:.1f}Â°")
                # OpenCV ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜í•´ í‘œì‹œ
                st.image(cv2.cvtColor(processed_img, cv2.COLOR_BGR2RGB), use_container_width=True)
            else:
                st.error("ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # [ì˜¤ë¥¸ìª½] ì‹¤ì‹œê°„ ì¹´ë©”ë¼
    with col2:
        st.subheader("2. ì‹¤ì‹œê°„ ë¶„ì„")
        
        # WebRTC ìŠ¤íŠ¸ë¦¬ë¨¸
        ctx = webrtc_streamer(
            key="angle-analysis",
            video_processor_factory=FaceAngleProcessor,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={
                "video": {"facingMode": "user"}, # ì „ë©´ ì¹´ë©”ë¼
                "audio": False
            },
            async_processing=True
        )

        # í”„ë¡œì„¸ì„œì— ê¸°ì¤€ ê°ë„ ì „ë‹¬
        if ctx.video_processor:
            ctx.video_processor.ref_angle = ref_angle_val

if __name__ == "__main__":
    main()
