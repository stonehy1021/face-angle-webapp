import cv2
import mediapipe as mp
import numpy as np
import av
import math
import time
import queue
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration

# ========= ê¸°ë³¸ ì„¤ì • =========
st.set_page_config(page_title="AI ìë™ ì´¬ì˜ê¸°", layout="wide")

# STUN ì„œë²„ (ë°°í¬ í•„ìˆ˜)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì°ì€ ì‚¬ì§„ ì €ì¥ìš©)
if "snapshot" not in st.session_state:
    st.session_state.snapshot = None

# Mediapipe ì´ˆê¸°í™”
mp_face = mp.solutions.face_detection

# ========= ìœ í‹¸ í•¨ìˆ˜ =========
def calc_roll_angle(detection, width, height):
    kp = detection.location_data.relative_keypoints
    left_eye = kp[0]
    right_eye = kp[1]
    x1, y1 = left_eye.x * width, left_eye.y * height
    x2, y2 = right_eye.x * width, right_eye.y * height
    angle = math.degrees(math.atan2(y2 - y1, x2 - x1))
    return angle

# ========= ì˜ìƒ ì²˜ë¦¬ í´ë˜ìŠ¤ (í•µì‹¬ ë¡œì§) =========
class FaceAngleProcessor(VideoProcessorBase):
    def __init__(self):
        self.ref_angle = None
        self.face_detector = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.5)
        
        # ì‚¬ì§„ ì „ì†¡ì„ ìœ„í•œ ìš°ì²´í†µ (Queue)
        self.result_queue = queue.Queue()
        
        # ìë™ ì´¬ì˜ìš© ë³€ìˆ˜
        self.match_start_time = None
        self.capture_cooldown = 0

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1) # ê±°ìš¸ ëª¨ë“œ
        h, w, _ = img.shape
        
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.face_detector.process(img_rgb)
        
        current_angle = 0.0
        status_text = "Looking..."
        color = (0, 0, 255) # ë¹¨ê°•

        if results.detections:
            detection = results.detections[0]
            current_angle = calc_roll_angle(detection, w, h)
            status_text = f"Angle: {current_angle:.1f}"

            if self.ref_angle is not None:
                diff = abs(current_angle - self.ref_angle)
                status_text += f" | Diff: {diff:.1f}"
                
                # â˜… ì´¬ì˜ ë¡œì§ â˜…
                # 1. ê°ë„ ì°¨ì´ê°€ 5ë„ ì´ë‚´ì¸ì§€ í™•ì¸
                if diff < 5.0:
                    color = (0, 255, 0) # ì´ˆë¡ìƒ‰
                    status_text = "HOLD ON!"
                    
                    # íƒ€ì´ë¨¸ ì‹œì‘
                    if self.match_start_time is None:
                        self.match_start_time = time.time()
                    
                    # 1ì´ˆ ë™ì•ˆ ìœ ì§€í•˜ë©´ ì´¬ì˜
                    if time.time() - self.match_start_time > 1.0:
                        # ì¿¨íƒ€ì„ ì²´í¬ (ì—°ì† ì´¬ì˜ ë°©ì§€)
                        if time.time() - self.capture_cooldown > 3.0:
                            # â˜… ì‚¬ì§„ ì°ì–´ì„œ ìš°ì²´í†µì— ë„£ê¸° â˜…
                            # (OpenCV ì´ë¯¸ì§€ëŠ” BGRì´ë¯€ë¡œ RGBë¡œ ë³€í™˜í•´ì„œ ë³´ëƒ„)
                            captured_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                            self.result_queue.put(captured_img)
                            
                            self.capture_cooldown = time.time()
                            status_text = "CAPTURED!"
                else:
                    # ì¡°ê±´ ì•ˆ ë§ìœ¼ë©´ íƒ€ì´ë¨¸ ë¦¬ì…‹
                    self.match_start_time = None
            
            # ì‹œê°í™”
            bbox = detection.location_data.relative_bounding_box
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            bw = int(bbox.width * w)
            bh = int(bbox.height * h)
            cv2.rectangle(img, (x, y), (x+bw, y+bh), color, 2)
            cv2.putText(img, status_text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
            
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# ========= ë©”ì¸ UI =========
def main():
    st.title("ğŸ“¸ AI ìë™ ì´¬ì˜ê¸°")
    st.info("ì™¼ìª½ì—ì„œ ì‚¬ì§„ì„ ì˜¬ë¦¬ê³ , ì˜¤ë¥¸ìª½ì—ì„œ ì¹´ë©”ë¼ë¥¼ ì¼œì„¸ìš”. ê°ë„ê°€ ë§ìœ¼ë©´ 1ì´ˆ ë’¤ ì°í™ë‹ˆë‹¤!")

    col1, col2 = st.columns([1, 1])

    # [ì™¼ìª½] ê¸°ì¤€ ì‚¬ì§„ ì—…ë¡œë“œ
    with col1:
        st.subheader("1. ê¸°ì¤€ ì‚¬ì§„")
        uploaded_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=['jpg', 'png', 'jpeg'])
        ref_angle_val = None
        
        if uploaded_file:
            file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
            ref_img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
            ref_img_rgb = cv2.cvtColor(ref_img, cv2.COLOR_BGR2RGB)
            
            with mp_face.FaceDetection(model_selection=1, min_detection_confidence=0.5) as detector:
                res = detector.process(ref_img_rgb)
                if res.detections:
                    h, w, _ = ref_img.shape
                    ref_angle_val = calc_roll_angle(res.detections[0], w, h)
                    st.success(f"ê¸°ì¤€ ê°ë„: {ref_angle_val:.1f}Â°")
                    st.image(ref_img_rgb, use_container_width=True)
                else:
                    st.error("ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # [ì˜¤ë¥¸ìª½] ì‹¤ì‹œê°„ ì¹´ë©”ë¼
    with col2:
        st.subheader("2. ì‹¤ì‹œê°„ ì´¬ì˜")
        
        # WebRTC ì‹¤í–‰
        ctx = webrtc_streamer(
            key="auto-capture",
            video_processor_factory=FaceAngleProcessor,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={"video": {"facingMode": "user"}, "audio": False},
            async_processing=True
        )

        # ê¸°ì¤€ ê°ë„ ì „ë‹¬
        if ctx.video_processor:
            ctx.video_processor.ref_angle = ref_angle_val

        # â˜… í•µì‹¬: ìš°ì²´í†µ(Queue) í™•ì¸í•˜ì—¬ ì‚¬ì§„ ê°€ì ¸ì˜¤ê¸° â˜…
        if ctx.state.playing:
            if ctx.video_processor:
                try:
                    # íì—ì„œ ì‚¬ì§„ì´ ì™”ë‚˜ í™•ì¸ (ë¸”ë¡œí‚¹ ì—†ì´)
                    result_image = ctx.video_processor.result_queue.get(timeout=0.1)
                    
                    # ì‚¬ì§„ì´ ë„ì°©í–ˆë‹¤ë©´ ì„¸ì…˜ì— ì €ì¥í•˜ê³  ì•± ìƒˆë¡œê³ ì¹¨
                    if result_image is not None:
                        st.session_state.snapshot = result_image
                        st.rerun()
                except queue.Empty:
                    pass

    # [í•˜ë‹¨] ê²°ê³¼ë¬¼ í‘œì‹œ ë° ë‹¤ìš´ë¡œë“œ
    st.markdown("---")
    if st.session_state.snapshot is not None:
        st.success("ğŸ‰ ì´¬ì˜ ì„±ê³µ! ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ì €ì¥í•˜ì„¸ìš”.")
        
        # ë³´ê¸° ì¢‹ê²Œ í‘œì‹œ
        st.image(st.session_state.snapshot, caption="ë°©ê¸ˆ ì°ì€ ì¸ìƒìƒ·", width=400)
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ìƒì„±
        # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
        img_bgr = cv2.cvtColor(st.session_state.snapshot, cv2.COLOR_RGB2BGR)
        ret, buffer = cv2.imencode('.jpg', img_bgr)
        
        if ret:
            st.download_button(
                label="ğŸ“¥ ë‚´ í°ì— ì €ì¥í•˜ê¸°",
                data=buffer.tobytes(),
                file_name="AI_Capture.jpg",
                mime="image/jpeg",
                type="primary"
            )
            
        if st.button("ë‹¤ì‹œ ì°ê¸°"):
            st.session_state.snapshot = None
            st.rerun()

if __name__ == "__main__":
    main()