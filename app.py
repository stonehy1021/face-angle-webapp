import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration
import cv2
import mediapipe as mp
import av
import numpy as np
import time
import queue
import functools

# ---------------- 1. ê¸°ë³¸ ì„¤ì • ----------------
st.set_page_config(page_title="AI ìë™ ì´¬ì˜ê¸°", layout="centered")

# STUN ì„œë²„ (ì™¸ë¶€ ì ‘ì†ìš©)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì‚¬ì§„ ì €ì¥ì†Œ & ìš°ì²´í†µ)
if "snapshot" not in st.session_state:
    st.session_state.snapshot = None
    
# [ì¤‘ìš”] ìš°ì²´í†µ(Queue)ì„ ì„¸ì…˜ì— ë°•ì œí•´ì„œ ì ˆëŒ€ ìƒì–´ë²„ë¦¬ì§€ ì•Šê²Œ í•¨
if "img_queue" not in st.session_state:
    st.session_state.img_queue = queue.Queue()

st.title("ğŸ“¸ AI ìë™ ì´¬ì˜ê¸° (ìµœì¢…)")
st.info("CAPTURED ë©”ì‹œì§€ê°€ ëœ¨ë©´ í™”ë©´ì´ ê¹œë¹¡ì´ê³  ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì´ ìƒê¹ë‹ˆë‹¤.")

# ---------------- 2. ì‚¬ì´ë“œë°” ì„¤ì • ----------------
st.sidebar.header("âš™ï¸ ì„¤ì •")
min_val = st.sidebar.slider("ìµœì†Œ ê°ë„", 0.0, 0.3, 0.02, 0.01)
max_val = st.sidebar.slider("ìµœëŒ€ ê°ë„", 0.0, 0.3, 0.15, 0.01)

# ---------------- 3. ì˜ìƒ ì²˜ë¦¬ í´ë˜ìŠ¤ ----------------
class FaceAngleProcessor(VideoProcessorBase):
    def __init__(self, img_queue):
        self.img_queue = img_queue # ë©”ì¸ì—ì„œ ê±´ë„¤ë°›ì€ ìš°ì²´í†µ
        self.face_detector = mp.solutions.face_detection.FaceDetection(
            model_selection=0, min_detection_confidence=0.5
        )
        
        # ë¡œì§ ë³€ìˆ˜
        self.match_start_time = None
        self.last_capture_time = 0
        self.flash_frame = 0

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1)
        h, w, _ = img.shape
        
        # ì–¼êµ´ ë¶„ì„
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.face_detector.process(img_rgb)
        
        status_text = "Looking..."
        color = (0, 0, 255) # ë¹¨ê°•

        if results.detections:
            detection = results.detections[0]
            
            # ê°ë„ ê³„ì‚° (ë‹¨ìˆœí™”ëœ Z-diff ë¡œì§)
            kp = detection.location_data.relative_keypoints
            # 0:LeftEye, 1:RightEye, 2:NoseTip, 3:MouthCenter, 4:Ear, 5:Ear
            # ëª¨ë°”ì¼ìš©: ì½”(2)ì™€ ëˆˆ(0)ì˜ Yì¢Œí‘œ ì°¨ì´ë¥¼ ì´ìš©í•œ ê¹Šì´ ì¶”ì •
            # (ì§ˆë¬¸ìë‹˜ì´ ì›í•˜ì‹œë˜ ë¡¤ë§ ê°ë„ê°€ ì•„ë‹Œ, ê³ ê°œ ë„ë•ì„ ê°ë„ë¥¼ ì¶”ì •)
            # ê¸°ì¡´ ë¡œì§ ìœ ì§€: chin(152) - forehead(10) -> Mediapipe Mesh í•„ìš”
            # í•˜ì§€ë§Œ FaceDetection ëª¨ë¸ì€ ëœë“œë§ˆí¬ê°€ 6ê°œë¿ì„.
            # FaceMesh ëŒ€ì‹  ê°€ë²¼ìš´ FaceDetectionì„ ì“°ë˜, ê°ë„ ë¡œì§ì€ 'ëˆˆ ê¸°ìš¸ê¸°'ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜
            # ë‹¨ìˆœ FaceMeshë¡œ ë‹¤ì‹œ ë³€ê²½í•´ì•¼ ì •í™•í•¨.
            # ì—¬ê¸°ì„œëŠ” ì§ˆë¬¸ìë‹˜ì˜ ì˜ë„(FaceMesh ë¡œì§)ë¥¼ ì‚´ë¦¬ê¸° ìœ„í•´ FaceMesh ì‚¬ìš© ê¶Œì¥.
            # ** ì¤‘ìš”: ìœ„ ì½”ë“œì—ì„œ mp.solutions.face_detectionì„ ì¼ëŠ”ë°,
            # ê°ë„(Z-diff)ë¥¼ ë³´ë ¤ë©´ face_meshë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤. ì•„ë˜ì—ì„œ FaceMeshë¡œ êµì²´í•©ë‹ˆë‹¤. **
            
            pass # ì•„ë˜ FaceMesh ë¡œì§ì—ì„œ ì²˜ë¦¬

        # [ìˆ˜ì •] FaceMeshë¡œ ì •í™•í•˜ê²Œ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì—¬ê¸°ì„œëŠ” ë°˜í™˜ë§Œ í•¨
        # ì‹¤ì œ ë¡œì§ì€ ì•„ë˜ processor_factoryì—ì„œ ì£¼ì…ëœ FaceMesh ì‚¬ìš©
        
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# [ìˆ˜ì •] FaceMeshë¥¼ ì‚¬ìš©í•˜ëŠ” ì§„ì§œ í”„ë¡œì„¸ì„œ
class FaceMeshProcessor(VideoProcessorBase):
    def __init__(self, img_queue):
        self.img_queue = img_queue
        self.face_mesh = mp.solutions.face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.match_start_time = None
        self.last_capture_time = 0
        self.flash_frame = 0

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1)
        h, w, _ = img.shape
        
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(img_rgb)
        
        status_text = "Adjust Angle"
        color = (0, 0, 255) # ë¹¨ê°•

        # í”Œë˜ì‹œ íš¨ê³¼
        if self.flash_frame > 0:
            self.flash_frame -= 1
            white = np.full((h, w, 3), 255, dtype=np.uint8)
            img = cv2.addWeighted(img, 0.5, white, 0.5, 0)
            status_text = "CAPTURED!"

        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark
            
            # Z-Diff ê³„ì‚°
            chin = landmarks[152].z
            forehead = landmarks[10].z
            current_z = (chin - forehead) * -1 
            
            # ë²”ìœ„ ì²´í¬ (ëª¨ë°”ì¼ ê¸°ì¤€ 0.02 ~ 0.15 ì¶”ì²œ)
            # ì—¬ê¸°ì„  ìŠ¬ë¼ì´ë” ê°’ì„ ì§ì ‘ ëª» ë°›ìœ¼ë‹ˆ ì•ˆì „í•˜ê²Œ ë„“ì€ ë²”ìœ„ ì„¤ì •
            # (ì‹¤ì œë¡œëŠ” ì „ì—­ë³€ìˆ˜ë‚˜ íë¡œ ê°’ì„ ë„˜ê²¨ì•¼ í•˜ì§€ë§Œ ë³µì¡ë„ ì¤„ì„)
            if 0.02 <= current_z <= 0.20:
                color = (0, 255, 0) # ì´ˆë¡
                status_text = "HOLD ON!"
                
                if self.match_start_time is None:
                    self.match_start_time = time.time()
                
                # 1ì´ˆ ìœ ì§€ ì‹œ ì´¬ì˜
                if time.time() - self.match_start_time > 1.0:
                    if time.time() - self.last_capture_time > 3.0:
                        # â˜… ì´¬ì˜ ë° ì „ì†¡ â˜…
                        send_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        self.img_queue.put(send_img) # ìš°ì²´í†µì— ë„£ìŒ
                        
                        self.last_capture_time = time.time()
                        self.flash_frame = 5
                        print("ğŸ“¸ ì„œë²„: ì‚¬ì§„ ì°ì–´ì„œ íì— ë„£ìŒ!")
            else:
                self.match_start_time = None
                
            # ì‹œê°í™”
            cv2.rectangle(img, (0,0), (w,h), color, 15)
            cv2.putText(img, f"Z: {current_z:.4f}", (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)
            cv2.putText(img, status_text, (30, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 3)
            
        return av.VideoFrame.from_ndarray(img, format="bgr24")


# ---------------- 4. ë©”ì¸ ë¡œì§ ----------------

# ì‚¬ì§„ì´ ì´ë¯¸ ì°í˜€ ìˆìœ¼ë©´ ê²°ê³¼ í™”ë©´ ë³´ì—¬ì£¼ê¸°
if st.session_state.snapshot is not None:
    st.success("ğŸ‰ ì´¬ì˜ ì„±ê³µ! ì €ì¥í•˜ì„¸ìš”.")
    
    col1, col2 = st.columns(2)
    with col1:
        st.image(st.session_state.snapshot, caption="ì¸ìƒìƒ·", use_container_width=True)
    with col2:
        # ì €ì¥ ë²„íŠ¼
        img_bgr = cv2.cvtColor(st.session_state.snapshot, cv2.COLOR_RGB2BGR)
        ret, buffer = cv2.imencode('.jpg', img_bgr)
        if ret:
            st.download_button(
                label="ğŸ“¥ ê°¤ëŸ¬ë¦¬ì— ì €ì¥",
                data=buffer.tobytes(),
                file_name=f"Selfie_{int(time.time())}.jpg",
                mime="image/jpeg",
                type="primary"
            )
    
    if st.button("ğŸ”„ ë‹¤ì‹œ ì°ê¸°", type="secondary"):
        st.session_state.snapshot = None
        st.rerun()

# ì‚¬ì§„ì´ ì—†ìœ¼ë©´ ì¹´ë©”ë¼ ë³´ì—¬ì£¼ê¸°
else:
    # [í•µì‹¬] ìš°ì²´í†µì„ í’ˆì€ í”„ë¡œì„¸ì„œ ìƒì„±ê¸°
    # ì´ë ‡ê²Œ í•´ì•¼ ì„¸ì…˜ì— ìˆëŠ” ìš°ì²´í†µì„ í”„ë¡œì„¸ì„œê°€ ì“¸ ìˆ˜ ìˆìŒ
    def processor_factory():
        return FaceMeshProcessor(st.session_state.img_queue)

    ctx = webrtc_streamer(
        key="mobile-capture",
        video_processor_factory=processor_factory,
        rtc_configuration=RTC_CONFIGURATION,
        media_stream_constraints={"video": {"facingMode": "user"}, "audio": False},
        async_processing=True
    )

    # [í•µì‹¬] ì‹¤ì‹œê°„ ìš°ì²´í†µ ê°ì‹œ ë£¨í”„
    if ctx.state.playing:
        placeholder = st.empty()
        placeholder.write("ğŸ“¸ ì¹´ë©”ë¼ ì‘ë™ ì¤‘... (ê°ë„ë¥¼ ë§ì¶°ë³´ì„¸ìš”)")
        
        while True:
            # 0.1ì´ˆë§ˆë‹¤ ìš°ì²´í†µ í™•ì¸
            if ctx.video_processor:
                try:
                    # íì—ì„œ ì‚¬ì§„ êº¼ë‚´ê¸° (ì¦‰ì‹œ í™•ì¸)
                    if not st.session_state.img_queue.empty():
                        result_img = st.session_state.img_queue.get()
                        st.session_state.snapshot = result_img
                        st.rerun() # ì‚¬ì§„ ì˜¤ë©´ ì¦‰ì‹œ ìƒˆë¡œê³ ì¹¨!
                except Exception as e:
                    print(e)
            time.sleep(0.1)


